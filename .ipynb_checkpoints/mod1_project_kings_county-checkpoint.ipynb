{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this project, we will be analysing the Kings county housing dataset\n",
    "Our objective is to use linear regression techniques to develop a model which can be used to predict the price of a home not already described in our original data set, based on the input of other data related to the house. \n",
    "\n",
    "Kings count is a county in Washington state. The western portion of it encompasses the city of Seattle. The data set contains sale information related to specific houes. It has around 21,000 records. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OBTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and dataset\n",
    "First off, lets import the modules we will require for this analysis, as well as the dataset itself. We will make a copy of the original dataframe which will serve as our working dataframe, while preserving a clean copy of the original dataset if needed. Lets also set some universal options at this stage as well. After that, let's take a quick look at the head of our primary dataframe to get a glimpse of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import statsmodels.api as sm\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from math import sqrt\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "pd.set_option(\"display.precision\", 8)\n",
    "mpl.rcParams['figure.figsize'] = (20,20)\n",
    "\n",
    "   \n",
    "df0 = pd.read_csv('kc_house_data.csv')   \n",
    "df1 = df0.copy()\n",
    "\n",
    "display(df1.head())\n",
    "display(df1.describe())\n",
    "print(type(df1.info()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Overview\n",
    "We will now gather some information about our data set. The function below takes the dataset and returns a dataframe\n",
    "with the following information for each column:\n",
    "    1. The data type of the columns\n",
    "    2. The missing values in the column, expressed as a percentage of the total records. \n",
    "    3. The number of unique values in the column\n",
    "    4. The most common value in column\n",
    "    5. The percentage of the column's values equal to the most common value in that column\n",
    "    6. The second, third...nth most common values and the percentage of the column that consists of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def report1 (dataframe,n_highest_counts):\n",
    "    \n",
    "    master={}\n",
    "        \n",
    "    for column in dataframe.columns:\n",
    "        \n",
    "        master[column]={}\n",
    "        col_dict = master[column]\n",
    "        col_dict['type'] = str(dataframe[column].dtypes)\n",
    "        col_dict['% empty'] = round(((len(dataframe)-dataframe[column].count())/len(dataframe))*100,2)\n",
    "        col_dict['unique values'] = dataframe[column].nunique()\n",
    "        \n",
    "        x = 1\n",
    "        series1 = dataframe[column].value_counts().head(n_highest_counts)\n",
    "        series1 = round((series1/len(dataframe)) * 100, 2)        \n",
    "        \n",
    "        for index,item in series1.items():\n",
    "            value_prop = str(x) + 'nth_value_%'\n",
    "            value_name = str(x) + 'nth_value'\n",
    "            col_dict[value_name] = index\n",
    "            col_dict[value_prop] = item\n",
    "            x += 1\n",
    "        \n",
    "    df_report=pd.DataFrame.from_dict(master,orient='index')\n",
    "    df_report.sort_values(['1nth_value_%'],ascending=False,inplace=True)\n",
    "        \n",
    "    return df_report\n",
    "\n",
    "df_report = report1(df1,10)\n",
    "display(df_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRUB\n",
    "Lets clean our dataset and get rid of useless fluff. To keep track of how much of the original data is left over, let us define a function which we can call after each step in the cleaning process to make sure we arent getting rid of too much data. This function will tell us what percent of the number of rows in the original dataframe still remain in our cleaned and modified dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_loss(df_base,df_current):\n",
    "    \n",
    "    rows_dropped = len(df_base) - len(df_current)\n",
    "    rows_left = len(df_current)\n",
    "    row_loss_perc = ((len(df_base) - rows_dropped)/len(df_base)) * 100\n",
    "    row_loss_perc = round(row_loss_perc,2)\n",
    "    \n",
    "    metrics = [rows_dropped,rows_left,row_loss_perc]\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def loss_report (df_base,df_current):\n",
    "    stats_list = row_loss(df_base,df_current)\n",
    "    \n",
    "    for x in stats_list:\n",
    "        x = str(x)\n",
    "      \n",
    "    string = \"Rows Dropped: {}    Rows Left: {}   Percentage Remaining: {}\".format(stats_list[0],stats_list[1],stats_list[2])\n",
    "    \n",
    "    print(string)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recasting Data Types\n",
    "We now take a closer look at which columns have a datatype of 'object', and whether we may turn them into numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_objects = list(df_report[df_report['type'] == 'object'].index)\n",
    "display(df_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets turn these into more appropriate data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "#df1['sqft_basement'] = pd.to_numeric(df1['sqft_basement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I commented out the line to recast sqft_basement. Turns out it has some problematic values in it. Lets hold off for now, we may end up dropping this column later for other reasons. \n",
    "\n",
    "As for the 'date' column, on second thought, lets drop it. We need to convert all our data into numeric form. We could conceivably do this with the date column but it would require some engineering. But if we think about it, the date column is probably not very useful to us in this case. It describes the date that the house in question was sold. I dont really see how this can have any significant effect on the price of a house. Lets at least drop it for now and perhaps we can retrain our model with a modified version of it later to see if it makes any difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sort_values('date',ascending=True)\n",
    "df1 = df1.drop(['date'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate Records\n",
    "\n",
    "We would like to assume that each row in the dataset refers to just one house. The 'id' column is described as being the unique identifier for a house. However, even though the dataset is 21597 records long, 'id' has 21420 unique values, and no missing values. This might indicate the existence of duplicates. \n",
    "\n",
    "The code below returns a list of id's that occur more than once in the dataframe. It then filters the dataframe to find all rows with those id numbers, sorts them by id number so we can observe the rows with the same Id numbers one on top of the other. If we see that the rest of the data in both rows with the same ID is also the same, we can confirm that these are true duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id_Counts = df1['id'].value_counts()\n",
    "repeats = df_id_Counts[df_id_Counts > 1]\n",
    "repeats = list(repeats.index)\n",
    "\n",
    "df_dups = df1[df1['id'].isin(repeats)]\n",
    "df_dups.sort_values(by='id',inplace=True)\n",
    "\n",
    "percent_of_duplicates = (len(df_dups) - len(repeats))/len(df1)\n",
    "percent_of_duplicates = round(percent_of_duplicates *100,2)\n",
    "                         \n",
    "print(\"{}% of the rows in the dataframe are duplicate records\".format(percent_of_duplicates))\n",
    "display(df_dups.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the duplicate ids refer to the same houses as they refer to houses with the same number of bedrooms, bathrooms, square footage etc. Given that only 0.82% of the rows of the dataset, we can safely discard them. Once we have done that, lets set the id number as the index of the dataframe, since now it truly is a unique identifies. This should help us with merges later on and maintain the integrity of our data. Lets also take a look at where our row loss metric stands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop_duplicates('id')\n",
    "df1.set_index('id',inplace=True)\n",
    "\n",
    "print(row_loss(df0,df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders and Missing Values\n",
    "\n",
    "Finding missing values is simple enough: we can use native python methods to examine the null values in a given dataframe or series. But as we know, oftentimes datasets have palceholder values that are actually null values, although not encoded as such. Finding these is trickier. One way is to look at the counts of unique values in each dataframe column to see if anything jumps out.\n",
    "\n",
    "Forunately, our custom report1 function will come in handy to find out if any particular values constitute and abormally large proportion of a gievn dataframe column. This could be an indication that there is a placeholder value that will skew our results. \n",
    "\n",
    "Lets take a look at the report again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_report)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "From the preceeding, we can make the following observations:\n",
    "\n",
    "1. \"View\" is a binary categorical variable, indicating whether a house has been viewed or not. 90% of the 'view' column has a value of '0', meaning 90% of the houses have not been viewed. This makes is highly likely that this column is not really providing us any useful information. We should probably drop it. \n",
    "\n",
    "2. Similary, 88% of 'waterfront' is also '0'. It seems most likely that waterfront is a binary categorical variable, i.e. a house either is not on the waterfront (in which case ot gets a 'o' value), or it is (in which case it gets '1'). Perhaps being on the waterfront makes a house more valuable, and if we build a model that takes into account whether a house is on the waterfront it may be more accurate. However, 11% of the data is missing. Lets fill those missing values with 0, since most houses likely are not waterfront adjacent. \n",
    "\n",
    "3. year renovated definitely seems like it should be deleted. 80% of the values are '0', which cannot be explained by the nature of the information in this column ('0' is not a year). And as we saw before, 17% of the values are missing. \n",
    "\n",
    "4. condition again seems to be a categorical variable. the values range from 1 to 5, and its probably a way of saying \"on a scale of 1 to 5 what is the condition of the property\". The most frequent value is 3, occuring 65% of the time, then 4 at 26% of the time. It tapers of before and after. If I had to guess, these values are normally distributed. 3 is right down the middle of the scale, and it makes sense that most houses would be in average condition, and more extreme conditions are less and less likely. We can double check this with a histogram. But we definitely want to retain condition in our feature set. \n",
    "\n",
    "5. sqft_basement should definitely go. Around 60% of the values are '0'. 2% are '?'. The remainign values are in increments of 100, which is highly unlikely for a quantity that one would expect to be more continuos and varying (compare with other square foot based columns). \n",
    "\n",
    "6. Around 50% of houses have only 1 floor. 40% have 2 floors. It goes down from there. This makes sense. Lets leave this feature in place. \n",
    "\n",
    "7. A large portion of houses(45%) have three bedrooms. 32% have 4 bedrooms. The rest have lesser or more. This also in line with what we would expect. 3 seems like the numebr of bedrooms we would expect most houses to have, so this is not unusual. Lets keep this feature\n",
    "\n",
    "8. 7 is the most common value for grade at 42%. Given that grade ranges from 3 to 13 and 7 is kind of in the middle, this also makes sense. Again, one would expect most houses to be graded average. \n",
    "\n",
    "9. Bathrooms also does not seem to have any glaring outliers, and seems in line with our expectations for the most common number of bathrooms across homes. \n",
    "\n",
    "Thus far, our list of features to remove looks like this:  \n",
    "\n",
    "        ['view','yr_renovated','sqft_basement']\n",
    "\n",
    "Lets drop these columns and return a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_null_columns = ['view','yr_renovated','sqft_basement']\n",
    "\n",
    "df1=df1.drop(high_null_columns,axis=1)\n",
    "df1['waterfront'] = df1['waterfront'].fillna(value=0)\n",
    "\n",
    "display(df1.head())\n",
    "print(df1.info())\n",
    "print(row_loss(df0,df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colinearity\n",
    "\n",
    "Lets check colinearity across all columns, to help us remove redundant features. \n",
    "A heatmap of correlation values will helpful to visually assess correlation among the features. \n",
    "\n",
    "The function below generates a clean, non-redundant heatmap of correlation values for the columns of our dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colinearity_plot(corr,figsize=(12,12)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    idx = np.triu_indices_from(mask)\n",
    "    mask[idx] = True\n",
    "\n",
    "    sns.heatmap(np.abs(corr),square=True,mask=mask,annot=True,cmap=\"Reds\",ax=ax)\n",
    "    return fig, ax\n",
    "\n",
    "colinearity_plot(np.abs(df1.corr()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some features a strongly co-related, and hence good candidates for removal on account of the multi-colinearity they introduce into the model. \n",
    "\n",
    "The function below returns a series consisting of pairs of columns with a Pearson correlation co-efficient higher than \n",
    "the specified threshold. The column pairs are displayed as elements of a multilevel index.\n",
    "\n",
    "We will use this function to return a list of feature pairs with a co-relation value higher than 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    " def high_corr(dataframe,threshold):\n",
    "    corr = dataframe.corr()\n",
    "    sign_corr = corr[abs(corr) > threshold]\n",
    "    sign_corr = sign_corr.stack()\n",
    "    sign_corr.drop_duplicates(inplace=True)\n",
    "    sign_corr = sign_corr[sign_corr != 1]\n",
    "    \n",
    "    \n",
    "    return sign_corr\n",
    "\n",
    "display(high_corr(df1,0.75))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "    Looks like sqft_living correlates with the highest number of other features. While it seems somewhat counter-intuitive to remove a metric as fundamental to evaluating home prices as square footage, on the other hand it seems the other features it co-relates to cover this information. Obviously higher bathrooms means higher square footage. But also, square_above seems to be a more on point data series. We are taking out basement squre footage anyways, and square above reflects this. So we may end up with a cleaner data set if we just take out square footage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colinear_columns = ['sqft_living']\n",
    "\n",
    "df1 = df1.drop(colinear_columns,axis=1)\n",
    "\n",
    "display(report1(df1,3))\n",
    "print(row_loss(df0,df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPLORE\n",
    "Lets begin our exploration of the dataset with a scatter mattrix, to get some insight into outliers and categorical variables. We will also make a copy of the dataframe as we have modified it thus far, because we may want to tweak what we do next in a future iteration of this process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df1,figsize=(20,20)); \n",
    "df_preproc = df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "I want to first remove outliers before scaling data. This seems like the correct approach, because I feel like scaling the data with outliers will produce skewed distributions which I will have to recorrect for outliers anyways, and end up removing the same outlying values I will now. I also want to use min-max scaling as I understand it is better at bringing data into more homogenous scales across columns.\n",
    "\n",
    "First lets draw boxplots for all our columns to identify which columns we should cull outliers from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_matrix (dataframe):\n",
    "    df1 = dataframe\n",
    "    col_nums = len(df1.columns)\n",
    "    if col_nums % 7 > 0:\n",
    "        chart_cols = int(round((col_nums/7)+1,0))\n",
    "    else:\n",
    "        chart_cols = int(round((col_nums/7),0))\n",
    "\n",
    "    figure, ax = plt.subplots(chart_cols,7,figsize=(15,15))\n",
    "\n",
    "    ax = ax.reshape(-1)\n",
    "\n",
    "    for i,col in enumerate(df1.columns):\n",
    "        df1.boxplot(column=col,ax=ax[i])\n",
    "     \n",
    "\n",
    "box_matrix(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these boxplots, we can identify some good candidates for culling outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cull_cols = ['sqft_lot15','price','sqft_lot','sqft_living15','bedrooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets write a function that culls values above a certain percentile for the columns we listed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def rm_outliers_dict (dataframe, culling_dict):\n",
    "    outliers_list = []\n",
    "    for col in culling_dict.keys():\n",
    "        outlier_indices = list(dataframe[dataframe[col] >= culling_dict[col]].index)\n",
    "        outliers_list = outliers_list + outlier_indices\n",
    "        \n",
    "    unique_set = set(outliers_list)\n",
    "    outliers_list = list(unique_set)\n",
    "    \n",
    "    dataframe = dataframe.drop(labels=outliers_list)        \n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def rm_outliers_threshold (dataframe, columns, threshold):\n",
    "    outliers_list = []\n",
    "    for col in columns:\n",
    "        outlier_indices = list(dataframe[dataframe[col] >= dataframe[col].quantile(threshold)].index)\n",
    "        outliers_list = outliers_list + outlier_indices\n",
    "        \n",
    "    unique_set = set(outliers_list)\n",
    "    outliers_list = list(unique_set)\n",
    "    \n",
    "    dataframe = dataframe.drop(labels=outliers_list) \n",
    "    return dataframe\n",
    "\n",
    "def cull_report (dataframe,columns,base_threshold,df_base):\n",
    "    \n",
    "    report_dict = {}\n",
    "        \n",
    "    y = int(base_threshold * 100)\n",
    "    \n",
    "    for x in range (100,y,-1):\n",
    "        \n",
    "        x = float(x/100)\n",
    "        \n",
    "        df = rm_outliers_threshold(dataframe,columns,x)\n",
    "        loss_metrics = row_loss(df_base,df)\n",
    "        report_dict[str(x)] = loss_metrics\n",
    "    \n",
    "    report_df = pd.DataFrame.from_dict(report_dict,orient='index',columns=['rows_dropped','rows_left','row_loss_perc'])\n",
    "    \n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culling_dict = {\n",
    "    'price':4000000,\n",
    "    'sqft_lot':500000,\n",
    "    'sqft_above':6000,\n",
    "    'sqft_living15':5300,\n",
    "    'sqft_lot15':400000\n",
    "}\n",
    "\n",
    "cull_cols = ['sqft_lot15','price','sqft_lot','sqft_living15']\n",
    "\n",
    "df11 = df1.copy()\n",
    "df12 = rm_outliers_dict(df11,culling_dict)\n",
    "df13 = cull_report(df12,cull_cols,.8,df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (20,6)\n",
    "plt.bar(df13.index,df13['rows_dropped'])\n",
    "plt.figure(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like removing rows containing a value that exceeds the .98 quantile for any column is a good place for us to take out the more extreme outliers without losing too much of our data. Lets drop those rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = rm_outliers_threshold (df1, cull_cols,.98)\n",
    "loss_report(df0,df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if this improved out scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df1,figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing features. \n",
    "Lets pull histograms of our features to get a sense of their distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.hist(bins=20,layout=(8,2),figsize=(15,25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these histograms we can see that the following are good candidates for normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_cols = ['bathrooms','sqft_above','sqft_living15','sqft_lot','sqft_lot15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should we normalize 'Price', given that its our target variable? Lets hold off for now and revisit. \n",
    "\n",
    "Below is a function that will normalize specified columns in a dataframe using np.log. Lets use it to normalize our "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logarize (dataframe,columns):\n",
    "    \n",
    "    df = dataframe\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] =df[col].map(lambda x: np.log(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "df1 = logarize(df1,norm_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.hist(bins=20,layout=(8,2),figsize=(15,25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "We must evaluate our features to see if any of them can rightly be treated as categorical variables, and then undertake the appropriate transformations of our data. From the scatter plots we drew before it seems that the following features might be categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_cols =  ['floors','condition','grade','zipcode','waterfront']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipcode definitely seems like a good candidate for a categorical variable to which we can apply one hot encoding. So does Waterfront. However, the remaining variables seem...inherently numeric, granted they dont have a large range of values. The number of floors a house has is still a number. Condition and grade are attempts to quantify qualitative features, and as such have already been converted to numeric features for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_cols =  ['zipcode','waterfront']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run one hot encoding on zipcode and then compile our final scaled dataframe. The function below returns a dataframe with the one-hot columns for the specified columns tacked on to the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_frame (dataframe,categ_cols):\n",
    "    for col in categ_cols:\n",
    "        cat_frame = pd.get_dummies(dataframe[col])\n",
    "        cat_frame = cat_frame.astype('int64')\n",
    "        \n",
    "        dataframe = dataframe.merge(cat_frame,left_index=True,right_index=True)\n",
    "        dataframe.drop(col,axis=1,inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "categ_cols = ['zipcode']\n",
    "df1 = category_frame(df1,categ_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its time to scale our features. I will use min-max, since I have already removed outliers. Perhaps after training my initial model i'll experiment with other scaling methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_col (series):\n",
    "    scaled = (series - min(series)) / (max(series) - min(series))\n",
    "    return scaled\n",
    "\n",
    "def df_scaler (dataframe):\n",
    "    for col in dataframe.columns:\n",
    "        dataframe[col] = min_max_col(dataframe[col])\n",
    "    return dataframe\n",
    "\n",
    "price = df1['price']\n",
    "df1.drop('price',axis=1,inplace=True)\n",
    "\n",
    "df1 = df_scaler(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 1: How does home price and value vary geographically?\n",
    "I would like to explore the variations in data across zipcodes in depth. It would be nice to have a geographic sense of house values, would'nt it? Lets import the GeoPands library to generate some map based visualizations to see how the different metrics in our dataset vary across geographic regions.\n",
    "\n",
    "Lets start by looking at what zipcodes have the highest median value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "df4 = df_preproc.copy()\n",
    "df4 = rm_outliers_threshold(df4,cull_cols,0.95)\n",
    "df_zipcode = df4.groupby('zipcode').median()\n",
    "\n",
    "df_zipcode.reset_index(level=0, inplace=True)\n",
    "\n",
    "king_count = gpd.read_file('king_county_shapefile/Zipcodes_for_King_County_and_Surrounding_Area_Shorelines__zipcode_shore_area.shp')\n",
    "\n",
    "king_count.drop(['OBJECTID','ZIP','ZIP_TYPE'],axis=1,inplace=True) \n",
    "king_count['ZIPCODE'] = pd.to_numeric(king_count['ZIPCODE'])\n",
    "king_count = king_count.merge(df_zipcode,left_on='ZIPCODE',right_on='zipcode',how='inner')\n",
    "\n",
    "king_count['coords']= king_count['geometry'].apply(lambda x : x.representative_point().coords[:])\n",
    "king_count['coords']= [coords[0] for coords in king_count['coords']]\n",
    "\n",
    "#dropping the western most zipcodes. Theyre sparsley populated, not very itneresting and is skewing out maps below\n",
    "king_count = king_count[king_count['long'] < -122.00]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price \n",
    "Lets start by looking at what zipcodes have the highest median sale price "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (30,30)\n",
    "\n",
    "title_dict = {'fontsize': 55,\n",
    " 'fontweight' : 15}\n",
    "\n",
    "king_count.plot(column='price',cmap='OrRd',legend=False)\n",
    "\n",
    "for idx, row in king_count.iterrows():\n",
    "    plt.annotate(s=row['ZIPCODE'], xy=row['coords'],\n",
    "                 horizontalalignment='center',size=20)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Price By Zipcode',fontdict=title_dict)\n",
    "\n",
    "plt.savefig('plot_images/price_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one would expect, the price of houses seems to get higher closer to the city center and drop as we head out. \n",
    "98040 and 98039 seem to have the most expensive properties! I wonder why that is?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price per square foot\n",
    "\n",
    "Let's engineer a new feature called \"Price/Sqft\" which will reflect the price per square foot. My hypothesis is that this metric will vary in tandem with geographic location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "king_count['price/sqft'] = king_count['price']/king_count['sqft_above']\n",
    "\n",
    "king_count.plot(column='price/sqft',cmap='OrRd',legend=False)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Price per Square Foot',fontdict=title_dict)\n",
    "\n",
    "for idx, row in king_count.iterrows():\n",
    "    plt.annotate(s=row['ZIPCODE'], xy=row['coords'],horizontalalignment='center',size=20)\n",
    "    \n",
    "plt.savefig('plot_images/price_sqft_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more stable metric and aligns closely with the proposition that living space is more valuable closer to the city center. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2: How are living space and lot size related, and how do they vary geographically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indoor Square Footage\n",
    "\n",
    "Now lets see how indoor square footage varies across the region. I suspect, it will vary in the opposite way as price, which is to say that indoor square footage per house will be smaller towards the city center and larger as we go out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_count.plot(column='sqft_above',cmap='OrRd',legend=False)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Indoor Square Footage',fontdict=title_dict)\n",
    "\n",
    "for idx, row in king_count.iterrows():\n",
    "    plt.annotate(s=row['ZIPCODE'], xy=row['coords'],horizontalalignment='center',size=20)\n",
    "    \n",
    "plt.savefig('plot_images/sqft_above_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough, our hypothesis checks out. Oh my 98075 looks like it has some huge mansions!It also seemed like the median price here was high too, relative to its neighbors.\n",
    "This must be a fancy neighborhood! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lot size\n",
    "Now lets see see how lot sizes change. I Imagine similarly to indooor square footage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_count.plot(column='sqft_lot',cmap='OrRd',legend=False)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Lot Size',fontdict=title_dict)\n",
    "\n",
    "for idx, row in king_count.iterrows():\n",
    "    plt.annotate(s=row['ZIPCODE'], xy=row['coords'],horizontalalignment='center',size=20)\n",
    "    \n",
    "plt.savefig('plot_images/lot_size_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lot size to Living Space ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 98075's neighbors to the north, 98074, have bigger yards but smaller houses. \n",
    "Lets engineer a new features called \"indoor/lot\". This will be ratio of the indoor square footage to the lot size. \n",
    "Basically, how much indoor space per unit of lot size. Lets make a new map for this feature. Might be a better way to get an estimate of living space density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "king_count['indoor/lot'] = king_count['sqft_above']/king_count['sqft_lot']\n",
    "\n",
    "king_count.plot(column='indoor/lot',cmap='OrRd',legend=False)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.title('Lot to Living Space Ratio',fontdict=title_dict)\n",
    "\n",
    "for idx, row in king_count.iterrows():\n",
    "    plt.annotate(s=row['ZIPCODE'], xy=row['coords'],horizontalalignment='center',size=20)\n",
    "    \n",
    "plt.savefig('plot_images/lot_vs_indoor_map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fascinating. As well see, there is more living space on each lot closer to the city center. This is probably due to the fact there are more apartment buildings closer to the city, so the living space per unit area is denser which is in line with what we expect from major cities. \n",
    "\n",
    "It also confirms that 98074 people do have larger yards, while 98075 folks have larger houses. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put together the final dataframe that we will feed into our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = price\n",
    "predictors = df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary OLS Model\n",
    "Lets fit our first model. We will start with OLS in statsmodels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_int = sm.add_constant(predictors)\n",
    "model = sm.OLS(target,pred_int).fit()\n",
    "\n",
    "display(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = model.pvalues\n",
    "series = series[series < 0.05]\n",
    "print(len(series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation \n",
    "Lets validate this model using train test split. We will split our dataset into chunks ranging from 5% of the dataset to 95% of the dataset, in increments of 5%. For each test, we will generate a hundered random splits of the data into training and test sets of the given test size. We will fit our model, use it to predict values on the test set and the gather the mean squared error for each iteration in the 25 random samples. We will take the average mean squared error from 25 samples at each test size and add them to a results data frame. In this way, we will be able to analyze the spread in the mean squared error between the training set and the test set, for different sizes of test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "mpl.rcParams['figure.figsize'] = (20,10)\n",
    "\n",
    "def test_size_validation (predictors,target):\n",
    "    collection = []\n",
    "    size = []\n",
    "    x= 0.05\n",
    "    \n",
    "    while x < 0.95:\n",
    "        \n",
    "        errorlist = []\n",
    "        \n",
    "        for j in range(1,50):\n",
    "            \n",
    "            x_train,x_test,y_train,y_test= train_test_split(predictors,target,test_size=x)\n",
    "            \n",
    "            x_train_int= sm.add_constant(x_train)\n",
    "            \n",
    "            x_test_int= sm.add_constant(x_test)\n",
    "            \n",
    "            olsmod = sm.OLS(y_train,x_train_int).fit()\n",
    "                       \n",
    "            y_train_hat = olsmod.predict(x_train_int)\n",
    "            \n",
    "            y_test_hat = olsmod.predict(x_test_int)\n",
    "            \n",
    "            train_mse = np.sum((y_train - y_train_hat)**2/len(y_train))\n",
    "\n",
    "            test_mse = np.sum((y_test - y_test_hat)**2/len(y_test))\n",
    "            \n",
    "            errorlist.append([train_mse,test_mse])\n",
    "\n",
    "\n",
    "        saveframe = pd.DataFrame(errorlist,columns=['train','test'])   \n",
    "        collection.append([str(x), round(saveframe['train'].mean(),3),round(saveframe['test'].mean(),3),0])   \n",
    "\n",
    "        x = round((x + 0.05),2)\n",
    "\n",
    "    coll_frame = pd.DataFrame(collection,columns=['size','train','test','delta%'])\n",
    "    coll_frame['delta%'] = ((coll_frame['test'] - coll_frame['train'])/coll_frame['train']) * 100\n",
    "    coll_frame['delta%'] = round(coll_frame['delta%'],2)\n",
    "    coll_frame.set_index('size',inplace=True)    \n",
    "    \n",
    "    return coll_frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets use this function to run a validation test on our dataset. We will also generate some visualizations to assess the impact of test size on the difference in mean squared error between the training and test sets at those test sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chart_train_test (predictors,target):\n",
    "    coll_frame = test_size_validation(predictors, target)\n",
    "\n",
    "    fig, ax = plt.subplots(2,1)\n",
    "\n",
    "    coll_frame.sort_index(ascending=True,inplace=True)\n",
    "\n",
    "    ax[0].scatter(coll_frame.index.values,coll_frame['train'],c='red')\n",
    "    ax[0].scatter(coll_frame.index.values,coll_frame['test'],c='blue')\n",
    "\n",
    "    coll_frame.sort_values('delta%',ascending=True,inplace=True)\n",
    "    ax[1].bar(coll_frame.index.values,coll_frame['delta%'])\n",
    "\n",
    "chart_train_test(predictors,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like a test data set size of 25% is a good size for splitting our dataset. Lets modify our function to validate OLS models by splitting the data up into these proportions randomly for 100 iterations and then gathering the mean squared error and the R-squared value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def mse_validation (predictors,target):\n",
    "    collection = []\n",
    "    x= 0.25            \n",
    "    errorlist = []\n",
    "\n",
    "    for j in range(1,25):\n",
    "\n",
    "        x_train,x_test,y_train,y_test= train_test_split(predictors,target,test_size=x)\n",
    "\n",
    "        x_train_int= sm.add_constant(x_train)\n",
    "\n",
    "        x_test_int= sm.add_constant(x_test)\n",
    "\n",
    "        olsmod = sm.OLS(y_train,x_train_int).fit()\n",
    "\n",
    "        y_train_hat = olsmod.predict(x_train_int)\n",
    "\n",
    "        y_test_hat = olsmod.predict(x_test_int)\n",
    "\n",
    "        train_mse = np.sum((y_train - y_train_hat)**2/len(y_train))\n",
    "\n",
    "        test_mse = np.sum((y_test - y_test_hat)**2/len(y_test))\n",
    "        \n",
    "        train_r2 = olsmod.rsquared\n",
    "        \n",
    "        train_rmse = sqrt(train_mse)\n",
    "        \n",
    "        test_rmse = sqrt(test_mse)\n",
    "\n",
    "        errorlist.append([train_mse,test_mse,train_r2,train_rmse,test_rmse])\n",
    "\n",
    "    saveframe = pd.DataFrame(errorlist,columns=['train','test','r2','train_rmse','test_rmse'])\n",
    "    \n",
    "    report_dict = {}\n",
    "    \n",
    "    report_dict['train_mean_squared_error'] = saveframe['train'].mean()\n",
    "    report_dict['test_mean_squared_error'] = saveframe['test'].mean()\n",
    "    report_dict['train_rmse'] = saveframe['train_rmse'].mean()\n",
    "    report_dict['test_rmse'] = saveframe['test_rmse'].mean()\n",
    "    report_dict['mean_r2'] = round(saveframe['r2'].mean(),2)\n",
    "    \n",
    "    report_frame = pd.DataFrame.from_dict(report_dict,orient='index',columns=['Scores'])  \n",
    "    \n",
    "    return report_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run our data set through this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "frame = mse_validation (predictors,target)\n",
    "\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we can improve these scores. But first lets make a model with SciKit Learn as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn Model\n",
    "Lets model using SciKit learn as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(predictors,target)\n",
    "\n",
    "print(linreg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Validation\n",
    "Lets use k fold cross validation to asses the Scikit learn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.metrics\n",
    "\n",
    "cv_10_results = np.mean(cross_val_score(linreg, predictors, target, cv=10, scoring='r2'))\n",
    "\n",
    "print(round(cv_10_results,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we can see this r2 is very close to the one we saw before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving model performance\n",
    "\n",
    "Let's see if we cant improve the performance of our model a little bit. Lets take a look at our coefficients to get an idea of which features have the most effect on the target variable. The following code returns a chart that shows us the magnitude of the coefficients for each feature stacked in order. A red bar indicated negative correlation, while a green bar indicates positive correlation. This way we can observe and assess the relative impact of each feature. We will use the OLS model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_charts (model):\n",
    "    main_features = model.params.iloc[:13].to_frame()\n",
    "    main_features.rename(columns={0:'coeffs'},inplace=True)\n",
    "    main_features.drop('const',axis=0,inplace=True)\n",
    "\n",
    "    main_features['magnitude'] = abs(main_features['coeffs'])\n",
    "    main_features['positive'] = main_features['coeffs'] < 0\n",
    "\n",
    "    main_features.sort_values(by='magnitude',ascending=False,inplace=True)\n",
    "    \n",
    "    main_features['coeffs'].plot(kind='barh',color=main_features.positive.map({True: 'r', False: 'g'}))\n",
    "   \n",
    "coef_charts(model)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lat and bedrooms dont impact our model much. Lets drop them and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors2 = predictors.drop(['lat','bedrooms'],axis=1)\n",
    "\n",
    "pred_int2 = sm.add_constant(predictors2)\n",
    "model2 = sm.OLS(target,pred_int2).fit()\n",
    "\n",
    "report_frame2 = mse_validation (predictors2,target)\n",
    "\n",
    "display(report_frame2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That didnt do much either. Maybe we should drop all columns with a p-value higher than 0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pvals (model,dataframe):\n",
    "    pvalues = round(model.pvalues,4)\n",
    "    pvalues = pvalues.drop('const')\n",
    "    high_pvalues = pvalues[pvalues < 0.05]    \n",
    "    high_list = list(high_pvalues.index)\n",
    "\n",
    "    dataframe = dataframe.drop(high_list,axis=1)\n",
    "    \n",
    "    return dataframe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = price.to_frame()\n",
    "\n",
    "predictors3 = remove_pvals(model2,df1)\n",
    "target = remove_pvals(model2,price)\n",
    "\n",
    "pred_int3 = sm.add_constant(predictors3)\n",
    "\n",
    "\n",
    "model3 = sm.OLS(target,pred_int3).fit()\n",
    "\n",
    "report_frame3 = mse_validation (predictors2,target)\n",
    "\n",
    "display(report_frame3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no luck :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we should think of other ways to improve our model. Perhaps we should revist our pre-scaled features and see if we should normalize any additional features. To help us do this, lets write a master function that will help us quickly tweak our data set. This function will rely on and consolidate the functions we previously wrote when we were exploring our data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_pre_dazzle (dataframe,dictionary):\n",
    "    \n",
    "    dict = dictionary\n",
    "    \n",
    "    dataframe = rm_outliers_threshold(dataframe,dict['cull_cols'],dict['threshold'])\n",
    "    \n",
    "    dataframe = logarize(dataframe,dict['norm_cols'])\n",
    "    \n",
    "    dataframe = category_frame(dataframe,dict['categ_cols'])\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def model_fit(dataframe,dictionary):\n",
    "    \n",
    "    dict = dictionary\n",
    "    \n",
    "    target = dataframe[dict['target']]\n",
    "    \n",
    "    predictors = df_scaler(dataframe.drop(dict['target'],axis=1))  \n",
    "        \n",
    "    pred_int = sm.add_constant(predictors)    \n",
    "    model = sm.OLS(target,pred_int).fit()\n",
    "        \n",
    "    return model\n",
    "\n",
    "def repre_fit (dataframe,dictionary):\n",
    "    \n",
    "    model_feed = re_pre_dazzle(dataframe,dictionary)\n",
    "    \n",
    "    model = model_fit(model_feed,dictionary)\n",
    "    \n",
    "    return model      \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will find the lists that we will feed into our master function to generate a new model. We can simply specify different culling thresholds and lists of columns to normalize and pull a new model. \n",
    "\n",
    "Lets lower the outlier threshold to 0.95 and normalize 'floors', in addition to the other columns we normalized before, and train a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = df_preproc.copy()\n",
    "\n",
    "pre = {}\n",
    "\n",
    "pre['threshold'] = 0.98\n",
    "pre['target'] = 'price'\n",
    "pre['cull_cols'] = ['sqft_lot15','price','sqft_lot','sqft_living15']\n",
    "pre['norm_cols'] = ['bathrooms','sqft_above','sqft_living15','sqft_lot','sqft_lot15','floors']\n",
    "pre['categ_cols'] = ['zipcode','waterfront'] \n",
    "\n",
    "\n",
    "model4 = repre_fit(df2,pre)\n",
    "\n",
    "display(model4.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our R squared went up a bit. Lets remove features above p = 0.05 and then do a train test split validation and see if this holds up, and get some mean error scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "df3 = df_preproc.copy()\n",
    "\n",
    "pre_proc_df = re_pre_dazzle(df3,pre)\n",
    "\n",
    "pre_proc_df = remove_pvals (model4,pre_proc_df)\n",
    "\n",
    "target = pre_proc_df['price']    \n",
    "predictors = df_scaler(pre_proc_df.drop('price',axis=1))\n",
    "\n",
    "frame = mse_validation (predictors,target)\n",
    "\n",
    "display(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictors.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like our r-squared stayed stable but out mean error scores went down quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Final Model.\n",
    "\n",
    "Our final model has an r-squared value of .8. This means that 80 percent of the variation above the mean price can be explained by our model, which is pretty decent for a regression model. \n",
    "\n",
    "Our mean squared error is around 89,000. This means that on average, a price we predict using our model is probably within 89,000 dollars of its actual value. Given that the mean price is around $500,000, this means the predicted prices will vary from the actual price by around 18% on average. Again, not bad for a regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTERPRET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3: Which features have the greatest effect on price?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, lets examine the coefficients from our updated model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_charts(model4)\n",
    "\n",
    "plt.savefig('plot_images/coeff_chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grade\n",
    "The most important factor appears to be grade, which would make sense because we can assume that 'grade' is assigned based on an overall assessment of the quality of the house and is essentially trying to do the same thing we are: come as close to evaluating the price of a house based on details about it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square Foot Above\n",
    "As we would expect, the square footage of a house and the lot its on has a huge effect on the price. Makes sense since thats the main quantity in housing that interests buyers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Built\n",
    "\n",
    "Year built seems to have an inverse relationship with price. This means that the older a house, the more expensive its likely to be. That might seem counter intuitive, but it makes sense once we take into account the fact that houses closer to the city were probably built first, and that since land close tot he city is scarce this drives up the value of those houses despite their age. On the other hand, newer development is more likely to occur towards the fringes of the region, where land is more plentiful on one hand and the locations are probably less desireable on account of the increased commute to the city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSIONS & RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the \"grade\" score corelates the most with the price. This makes sense because the grade system is probably meant to reflect the value of the house and is essentially an analogue version of our linear regression model. After that, above ground square footage of living space is the next most significant factor. This also makes sense with the fundamental mechanics of the real estate market: bigger houses are more expensive. \n",
    "\n",
    "We also saw that that generally speaking, the overall price and price per square foot of living space decreases as you move further east and away from the city center. We also saw that two neighborhood (98039 and 98040) seem to be the most high end of them all. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": "1",
   "nav_menu": {
    "height": "624px",
    "width": "448px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "623.535px",
    "left": "1988.05px",
    "top": "65.4688px",
    "width": "246.152px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
